{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDgzYbpRusU2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio onnx2torch albumentations pandas scikit-learn opencv-python-headless matplotlib"
      ],
      "metadata": {
        "id": "xUu2-1A_u9bH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing all the libraries\n"
      ],
      "metadata": {
        "id": "lDHKgQ5SvAP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from onnx2torch import convert\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "5tPmzJKOu94S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#reading the dataset"
      ],
      "metadata": {
        "id": "I20xVi9YvFds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/GestaltMatcher/data/GestaltMatcherDB/v1.1.0\"\n",
        "IMG_DIR = os.path.join(BASE_DIR, \"gmdb_align\")\n",
        "META_DIR = os.path.join(BASE_DIR, \"gmdb_metadata\")\n",
        "\n",
        "image_metadata = pd.read_csv(os.path.join(META_DIR, \"image_metadata_v1.1.0.tsv\"), sep='\\t')\n",
        "train_df = pd.read_csv(os.path.join(META_DIR, \"gmdb_train_images_v1.1.0.csv\"))\n",
        "val_df = pd.read_csv(os.path.join(META_DIR, \"gmdb_val_images_v1.1.0.csv\"))\n",
        "frequent_gallery_df = pd.read_csv(os.path.join(META_DIR, \"gmdb_frequent_gallery_images_v1.1.0.csv\"))\n",
        "frequent_test_df = pd.read_csv(os.path.join(META_DIR, \"gmdb_frequent_test_images_v1.1.0.csv\"))\n",
        "\n",
        "id_to_syndrome = image_metadata.set_index('image_id')['internal_syndrome_name'].to_dict()\n",
        "\n",
        "train_df['syndrome_name'] = train_df['image_id'].map(id_to_syndrome)\n",
        "val_df['syndrome_name'] = val_df['image_id'].map(id_to_syndrome)\n",
        "frequent_gallery_df['syndrome_name'] = frequent_gallery_df['image_id'].map(id_to_syndrome)\n",
        "frequent_test_df['syndrome_name'] = frequent_test_df['image_id'].map(id_to_syndrome)\n",
        "\n",
        "syndrome_counts = train_df['syndrome_name'].value_counts()\n",
        "frequent_syndromes = syndrome_counts[syndrome_counts > 1].index\n",
        "\n",
        "train_df = train_df[train_df['syndrome_name'].isin(frequent_syndromes)]\n",
        "val_df = val_df[val_df['syndrome_name'].isin(frequent_syndromes)]\n",
        "frequent_gallery_df = frequent_gallery_df[frequent_gallery_df['syndrome_name'].isin(frequent_syndromes)]\n",
        "frequent_test_df = frequent_test_df[frequent_test_df['syndrome_name'].isin(frequent_syndromes)]\n",
        "\n",
        "print(\"Training Data:\", train_df.head(), sep='\\n')\n",
        "print(\"\\nValidation Data:\", val_df.head(), sep='\\n')\n",
        "print(\"\\nFrequent Gallery Data:\", frequent_gallery_df.head(), sep='\\n')\n",
        "print(\"\\nFrequent Test Data:\", frequent_test_df.head(), sep='\\n')"
      ],
      "metadata": {
        "id": "uCwsVLM7vFA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#defining of loading arcface weights into pytorch model"
      ],
      "metadata": {
        "id": "_oRdVoH5vX-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from onnx2torch import convert\n",
        "\n",
        "class ArcFaceClassifier(nn.Module):\n",
        "    def __init__(self, backbone_path, num_classes, device='cuda'):\n",
        "        super(ArcFaceClassifier, self).__init__()\n",
        "        self.backbone = convert(backbone_path).to(device)\n",
        "        self.classifier = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            features = self.backbone(x)\n",
        "        out = self.classifier(features)\n",
        "        return out"
      ],
      "metadata": {
        "id": "fP21QBSmvYKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#loading downloaded models in pytorch"
      ],
      "metadata": {
        "id": "FNxAkKQ8vikb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(train_df['syndrome_name'].unique())\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "arcface_r50_path = \"/content/drive/MyDrive/GestaltMatcher/saved_models/glint360k_r50.onnx\"\n",
        "arcface_r100_path = \"/content/drive/MyDrive/GestaltMatcher/saved_models/glint360k_r100.onnx\"\n",
        "\n",
        "model_r50 = ArcFaceClassifier(arcface_r50_path, num_classes, device=device).to(device)\n",
        "model_r100 = ArcFaceClassifier(arcface_r100_path, num_classes, device=device).to(device)\n",
        "\n",
        "# print(model_r50)\n",
        "# print(model_r100)"
      ],
      "metadata": {
        "id": "zKyEBqZYvlAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#augmenting images, (horizontal flip, color jitter,random scaling, resize and normalize"
      ],
      "metadata": {
        "id": "adiKTRC8vxYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "train_transforms = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
        "    A.RandomScale(scale_limit=0.2, p=0.3),\n",
        "    A.Resize(112, 112),\n",
        "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "val_transforms = A.Compose([\n",
        "    A.Resize(112, 112),\n",
        "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "    ToTensorV2()\n",
        "])"
      ],
      "metadata": {
        "id": "4yr_UFitvxtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#making dataset for gmdb"
      ],
      "metadata": {
        "id": "1S8O_Ahpv4H5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "class GMDBDataset(Dataset):\n",
        "    def __init__(self, dataframe, img_dir, transforms=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.img_dir = img_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.dataframe.iloc[idx]['image_id']\n",
        "        img_path = os.path.join(self.img_dir, f\"{img_id}_aligned.jpg\")\n",
        "        image = cv2.imread(img_path)\n",
        "        if image is None:\n",
        "            print(f\"Image not found {img_path}\")\n",
        "        else:\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            label = self.dataframe.iloc[idx]['label']\n",
        "            if self.transforms:\n",
        "                augmented = self.transforms(image=image)\n",
        "                image = augmented['image']\n",
        "            return image, label"
      ],
      "metadata": {
        "id": "mmRnW_Ubv58u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#partitioning dataset into training and validation"
      ],
      "metadata": {
        "id": "tVNhDGfgwBm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "img_dir = \"/content/drive/MyDrive/GestaltMatcher/data/GestaltMatcherDB/v1.1.0/gmdb_align\"\n",
        "\n",
        "train_dataset = GMDBDataset(train_df, img_dir, transforms=train_transforms)\n",
        "val_dataset = GMDBDataset(val_df, img_dir, transforms=val_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "z8FOe2KrwEcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#training and saving the r50 model"
      ],
      "metadata": {
        "id": "fGzkLH1GwKT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "num_epochs = 30\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "checkpoint_dir = \"/content/drive/MyDrive/GestaltMatcher/checkpoints\"\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_r50.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "writer = SummaryWriter(comment=f\"model_r50_training_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\")\n",
        "\n",
        "def save_checkpoint(epoch, model, optimizer, scheduler, path):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict()\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "    print(f\"Checkpoint saved at epoch {epoch} to {path}\")\n",
        "\n",
        "def load_checkpoint(model, optimizer, scheduler, path):\n",
        "    if os.path.exists(path):\n",
        "        checkpoint = torch.load(path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        print(f\"Resuming training from epoch {start_epoch}\")\n",
        "        return start_epoch\n",
        "    else:\n",
        "        print(\"No checkpoint found, starting from scratch\")\n",
        "        return 0\n",
        "\n",
        "checkpoint_path = os.path.join(checkpoint_dir, \"model_r50_checkpoint.pth\")\n",
        "start_epoch = load_checkpoint(model_r50, optimizer, scheduler, checkpoint_path)\n",
        "\n",
        "def top_n_accuracy(output, target, n=5):\n",
        "    _, pred = output.topk(n, 1, True, True)\n",
        "    correct = pred.eq(target.view(-1, 1).expand_as(pred))\n",
        "    return correct.float().sum().item() / target.size(0)\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    model_r50.train()\n",
        "    running_loss = 0.0\n",
        "    correct_top1 = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_r50(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        running_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct_top1 += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    train_top1_acc = correct_top1 / total\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Top-1 Accuracy: {train_top1_acc:.4f}\")\n",
        "\n",
        "    writer.add_scalar('Training/Loss', avg_loss, epoch)\n",
        "    writer.add_scalar('Training/Top-1 Accuracy', train_top1_acc, epoch)\n",
        "\n",
        "    model_r50.eval()\n",
        "    val_loss = 0.0\n",
        "    val_top1_correct = 0\n",
        "    val_total = 0\n",
        "    val_top5_correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model_r50(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = outputs.max(1)\n",
        "            val_top1_correct += predicted.eq(labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "            val_top5_correct += top_n_accuracy(outputs, labels, n=5) * labels.size(0)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_top1_acc = val_top1_correct / val_total\n",
        "    val_top5_acc = val_top5_correct / val_total\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}, Top-1 Accuracy: {val_top1_acc:.4f}, Top-5 Accuracy: {val_top5_acc:.4f}\")\n",
        "\n",
        "    writer.add_scalar('Validation/Loss', avg_val_loss, epoch)\n",
        "    writer.add_scalar('Validation/Top-1 Accuracy', val_top1_acc, epoch)\n",
        "    writer.add_scalar('Validation/Top-5 Accuracy', val_top5_acc, epoch)\n",
        "\n",
        "    scheduler.step(val_top1_acc)\n",
        "    save_checkpoint(epoch, model_r50, optimizer, scheduler, checkpoint_path)\n",
        "\n",
        "final_model_path = \"/content/drive/MyDrive/GestaltMatcher/saved_models/model_r50_trained_final.pth\"\n",
        "torch.save(model_r50.state_dict(), final_model_path)\n",
        "print(f\"Final model saved to {final_model_path}\")\n",
        "\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "AShSc9ZWwMCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#training and saving the r100 model"
      ],
      "metadata": {
        "id": "btOWbK1Cwco6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def top_n_accuracy(output, target, n=5):\n",
        "    with torch.no_grad():\n",
        "        top_n_preds = output.topk(n, dim=1).indices\n",
        "        correct = top_n_preds.eq(target.view(-1, 1).expand_as(top_n_preds))\n",
        "        top_n_acc = correct.any(dim=1).float().mean().item()\n",
        "    return top_n_acc\n",
        "\n",
        "import torch\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "num_epochs_r100 = 30\n",
        "batch_size_r100 = 32\n",
        "learning_rate_r100 = 0.001\n",
        "checkpoint_dir_r100 = \"/content/drive/MyDrive/GestaltMatcher/checkpoints\"\n",
        "\n",
        "writer_r100 = SummaryWriter(log_dir=\"/content/drive/MyDrive/GestaltMatcher/tensorboard_logs/model_r100\")\n",
        "\n",
        "criterion_r100 = nn.CrossEntropyLoss()\n",
        "optimizer_r100 = optim.Adam(model_r100.parameters(), lr=learning_rate_r100)\n",
        "scheduler_r100 = optim.lr_scheduler.ReduceLROnPlateau(optimizer_r100, mode='max', factor=0.5, patience=5)\n",
        "\n",
        "def save_checkpoint_r100(epoch, model, optimizer, scheduler, path):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'best_val_accuracy': best_val_accuracy\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "\n",
        "checkpoint_path_r100 = os.path.join(checkpoint_dir_r100, \"model_r100_checkpoint.pth\")\n",
        "if os.path.exists(checkpoint_path_r100):\n",
        "    checkpoint = torch.load(checkpoint_path_r100)\n",
        "    model_r100.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer_r100.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler_r100.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    start_epoch_r100 = checkpoint['epoch'] + 1\n",
        "    best_val_accuracy = checkpoint.get('best_val_accuracy', 0)\n",
        "else:\n",
        "    start_epoch_r100 = 0\n",
        "    best_val_accuracy = 0\n",
        "\n",
        "for epoch in range(start_epoch_r100, num_epochs_r100):\n",
        "    model_r100.train()\n",
        "    running_loss = 0.0\n",
        "    correct_top1 = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer_r100.zero_grad()\n",
        "        outputs = model_r100(images)\n",
        "        loss = criterion_r100(outputs, labels)\n",
        "        running_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer_r100.step()\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct_top1 += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    train_top1_acc = correct_top1 / total\n",
        "\n",
        "    writer_r100.add_scalar('Training/Loss', avg_loss, epoch)\n",
        "    writer_r100.add_scalar('Training/Top-1 Accuracy', train_top1_acc, epoch)\n",
        "\n",
        "    model_r100.eval()\n",
        "    val_loss = 0.0\n",
        "    val_top1_correct = 0\n",
        "    val_total = 0\n",
        "    val_top5_correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model_r100(images)\n",
        "            loss = criterion_r100(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            val_top1_correct += predicted.eq(labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "            val_top5_correct += top_n_accuracy(outputs, labels, n=5) * labels.size(0)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_top1_acc = val_top1_correct / val_total\n",
        "    val_top5_acc = val_top5_correct / val_total\n",
        "\n",
        "    writer_r100.add_scalar('Validation/Loss', avg_val_loss, epoch)\n",
        "    writer_r100.add_scalar('Validation/Top-1 Accuracy', val_top1_acc, epoch)\n",
        "    writer_r100.add_scalar('Validation/Top-5 Accuracy', val_top5_acc, epoch)\n",
        "\n",
        "    scheduler_r100.step(val_top1_acc)\n",
        "\n",
        "    if val_top1_acc > best_val_accuracy:\n",
        "        best_val_accuracy = val_top1_acc\n",
        "        save_checkpoint_r100(epoch, model_r100, optimizer_r100, scheduler_r100, checkpoint_path_r100)\n",
        "\n",
        "final_model_path_r100 = \"/content/drive/MyDrive/GestaltMatcher/saved_models/model_r100_trained_final.pth\"\n",
        "torch.save(model_r100.state_dict(), final_model_path_r100)\n",
        "writer_r100.close()"
      ],
      "metadata": {
        "id": "rNTrvOUywezE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#loading saved r50 model"
      ],
      "metadata": {
        "id": "hEWDZxiywtan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "arcface_r50_path = \"/content/drive/MyDrive/GestaltMatcher/saved_models/glint360k_r50.onnx\"\n",
        "num_classes = len(train_df['syndrome_name'].unique())\n",
        "\n",
        "model_r50 = ArcFaceClassifier(arcface_r50_path, num_classes, device=device).to(device)\n",
        "\n",
        "final_model_path_r50 = \"/content/drive/MyDrive/GestaltMatcher/saved_models/model_r50_trained_final_new.pth\"\n",
        "model_r50.load_state_dict(torch.load(final_model_path_r50, map_location=device))\n",
        "\n",
        "model_r50.eval()"
      ],
      "metadata": {
        "id": "_e1JaU9Iwyv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#loading r100 model"
      ],
      "metadata": {
        "id": "KSNahIgdw5b5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "arcface_r100_path = \"/content/drive/MyDrive/GestaltMatcher/saved_models/glint360k_r100.onnx\"\n",
        "num_classes = len(train_df['syndrome_name'].unique())\n",
        "\n",
        "model_r100 = ArcFaceClassifier(arcface_r100_path, num_classes, device=device).to(device)\n",
        "\n",
        "final_model_path_r100 = \"/content/drive/MyDrive/GestaltMatcher/saved_models/model_r100_trained_final_new.pth\"\n",
        "model_r100.load_state_dict(torch.load(final_model_path_r100, map_location=device))\n",
        "\n",
        "model_r100.eval()"
      ],
      "metadata": {
        "id": "0OcX0ocCw67p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#making and saving encondings"
      ],
      "metadata": {
        "id": "6XJgmewixENv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from albumentations import Compose, Normalize, HorizontalFlip, Resize\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "\n",
        "IMG_DIR = \"/content/drive/MyDrive/GestaltMatcher/data/GestaltMatcherDB/v1.1.0/gmdb_align\"\n",
        "OUTPUT_CSV = \"/content/drive/MyDrive/GestaltMatcher/data/GestaltMatcherDB/v1.1.0/gmdb_align/all_encodings_new.csv\"\n",
        "MODEL_R50_PATH = \"/content/drive/MyDrive/GestaltMatcher/saved_models/model_r50_trained_final.pth\"\n",
        "MODEL_R100_PATH = \"/content/drive/MyDrive/GestaltMatcher/saved_models/model_r100_trained_final.pth\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "base_transforms = Compose([\n",
        "    Resize(112, 112),\n",
        "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "flip_transforms = Compose([\n",
        "    HorizontalFlip(p=1.0),\n",
        "    Resize(112, 112),\n",
        "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "def preprocess_image(image_path, transforms):\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    augmented = transforms(image=image)\n",
        "    return augmented['image'].unsqueeze(0)\n",
        "\n",
        "def generate_encodings(models, image_tensor):\n",
        "    encodings = []\n",
        "    for model in models:\n",
        "        with torch.no_grad():\n",
        "            encoding = model(image_tensor.to(device)).cpu().numpy()\n",
        "            encodings.append(encoding.squeeze())\n",
        "    return encodings\n",
        "\n",
        "output_data = []\n",
        "image_files = sorted([f for f in os.listdir(IMG_DIR) if f.endswith('_aligned.jpg')])\n",
        "\n",
        "for img_file in tqdm(image_files, desc=\"Generating Encodings\"):\n",
        "    img_path = os.path.join(IMG_DIR, img_file)\n",
        "    img_name = img_file.split('_')[0]\n",
        "\n",
        "    img_tensor = preprocess_image(img_path, base_transforms)\n",
        "    original_encodings = generate_encodings([model_r50, model_r100], img_tensor)\n",
        "\n",
        "    flipped_tensor = preprocess_image(img_path, flip_transforms)\n",
        "    flipped_encodings = generate_encodings([model_r50, model_r100], flipped_tensor)\n",
        "\n",
        "    avg_encodings = [\n",
        "        np.mean([orig, flip], axis=0)\n",
        "        for orig, flip in zip(original_encodings, flipped_encodings)\n",
        "    ]\n",
        "\n",
        "    for idx, (orig, flip, avg) in enumerate(zip(original_encodings, flipped_encodings, avg_encodings)):\n",
        "        output_data.append({\n",
        "            \"img_name\": img_name,\n",
        "            \"model\": f\"model_r{50 if idx == 0 else 100}\",\n",
        "            \"flip\": int(idx == 1),\n",
        "            \"class_conf\": np.max(avg),\n",
        "            \"representations\": avg.tolist()\n",
        "        })\n",
        "\n",
        "output_df = pd.DataFrame(output_data)\n",
        "output_df.to_csv(OUTPUT_CSV, index=False)\n",
        "\n",
        "print(f\"Encodings saved to {OUTPUT_CSV}\")"
      ],
      "metadata": {
        "id": "kckovPbYxF6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "P1EorB07xPAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "from ast import literal_eval\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/GestaltMatcher/data/GestaltMatcherDB/v1.1.0\"\n",
        "ENCODINGS_PATH = os.path.join(BASE_DIR, \"gmdb_align/all_encodings.csv\")\n",
        "META_DIR = os.path.join(BASE_DIR, \"gmdb_metadata\")\n",
        "\n",
        "encodings_df = pd.read_csv(ENCODINGS_PATH)\n",
        "frequent_gallery_df = pd.read_csv(os.path.join(META_DIR, \"gmdb_frequent_gallery_images_v1.1.0.csv\"))\n",
        "frequent_test_df = pd.read_csv(os.path.join(META_DIR, \"gmdb_frequent_test_images_v1.1.0.csv\"))\n",
        "\n",
        "encodings_df['representations'] = encodings_df['representations'].apply(literal_eval)\n",
        "\n",
        "def prepare_encodings(df, encodings_df):\n",
        "    print(\"frequent_gallery_df columns:\", df.columns)\n",
        "    print(\"encodings_df columns:\", encodings_df.columns)\n",
        "\n",
        "    merged_df = df.merge(encodings_df, left_on=\"image_id\", right_on=\"img_name\", how=\"inner\")\n",
        "\n",
        "    print(\"Merged DataFrame columns:\", merged_df.columns)\n",
        "\n",
        "    if 'representations_y' in merged_df.columns:\n",
        "        merged_df[\"representations\"] = merged_df[\"representations_y\"].apply(lambda x: np.array(x) if isinstance(x, list) else np.array(eval(x)))\n",
        "    elif 'representations' in merged_df.columns:\n",
        "        merged_df[\"representations\"] = merged_df[\"representations\"].apply(lambda x: np.array(x) if isinstance(x, list) else np.array(eval(x)))\n",
        "    else:\n",
        "        raise KeyError(\"'representations' column is missing in the merged DataFrame.\")\n",
        "\n",
        "    return merged_df\n",
        "\n",
        "gallery = prepare_encodings(frequent_gallery_df, encodings_df)\n",
        "test_set = prepare_encodings(frequent_test_df, encodings_df)\n",
        "\n",
        "gallery[\"syndrome_name\"] = gallery[\"label\"]\n",
        "test_set[\"syndrome_name\"] = test_set[\"label\"]\n",
        "\n",
        "def compute_top_n_accuracy(similarities, gallery_labels, test_labels, n=1):\n",
        "    correct = 0\n",
        "    for i, test_label in enumerate(test_labels):\n",
        "        top_n_indices = np.argsort(similarities[i])[::-1][:n]\n",
        "        top_n_labels = gallery_labels[top_n_indices]\n",
        "        if test_label in top_n_labels:\n",
        "            correct += 1\n",
        "    return correct / len(test_labels)\n",
        "\n",
        "def evaluate_model(gallery, test_set, top_ns=[1, 5, 10, 30]):\n",
        "    gallery_encodings = np.vstack(gallery[\"representations\"].values)\n",
        "    gallery_labels = gallery[\"syndrome_name\"].values\n",
        "    test_encodings = np.vstack(test_set[\"representations\"].values)\n",
        "    test_labels = test_set[\"syndrome_name\"].values\n",
        "\n",
        "    similarities = cosine_similarity(test_encodings, gallery_encodings)\n",
        "\n",
        "    top_n_results = {}\n",
        "    for n in top_ns:\n",
        "        top_n_results[f\"Top-{n}\"] = compute_top_n_accuracy(similarities, gallery_labels, test_labels, n=n)\n",
        "    return top_n_results\n",
        "\n",
        "print(\"Evaluating model ensemble...\")\n",
        "results_r50 = evaluate_model(gallery, test_set)\n",
        "results_r100 = evaluate_model(gallery, test_set)\n",
        "\n",
        "def evaluate_ensemble(gallery, test_set, encodings_df, top_ns=[1, 5, 10, 30]):\n",
        "    print(\"start r_50 encodings_df..\")\n",
        "    r50_encodings = encodings_df[encodings_df[\"model\"] == \"model_r50\"]\n",
        "    print(\"start r_100 encodings_df..\")\n",
        "    r100_encodings = encodings_df[encodings_df[\"model\"] == \"model_r100\"]\n",
        "\n",
        "    print(\"start r_50 prepare encodings..\")\n",
        "    r50_test = prepare_encodings(test_set, r50_encodings)\n",
        "    print(\"start r_100 prepare encodings..\")\n",
        "    r100_test = prepare_encodings(test_set, r100_encodings)\n",
        "    combined_test_encodings = (np.vstack(r50_test[\"representations\"].values) +\n",
        "                               np.vstack(r100_test[\"representations\"].values)) / 2\n",
        "\n",
        "    gallery_encodings = np.vstack(gallery[\"representations\"].values)\n",
        "    gallery_labels = gallery[\"syndrome_name\"].values\n",
        "    similarities = cosine_similarity(combined_test_encodings, gallery_encodings)\n",
        "\n",
        "    top_n_results = {}\n",
        "    for n in top_ns:\n",
        "        top_n_results[f\"Top-{n}\"] = compute_top_n_accuracy(similarities, gallery_labels, test_set[\"syndrome_name\"].values, n=n)\n",
        "    return top_n_results\n",
        "\n",
        "ensemble_results = evaluate_ensemble(gallery, test_set, encodings_df)\n",
        "\n",
        "print(\"Results for model_r50:\", results_r50)\n",
        "print(\"Results for model_r100:\", results_r100)\n",
        "print(\"Ensemble Results:\", ensemble_results)\n",
        "\n",
        "results_df = pd.DataFrame([\n",
        "    {\"Model\": \"model_r50\", **results_r50},\n",
        "    {\"Model\": \"model_r100\", **results_r100},\n",
        "    {\"Model\": \"Ensemble\", **ensemble_results}\n",
        "])\n",
        "results_df.to_csv(\"evaluation_results.csv\", index=False)\n",
        "print(\"Evaluation results saved to evaluation_results.csv.\")"
      ],
      "metadata": {
        "id": "lzg5aK9xxkhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#adding syndrom names to encoding"
      ],
      "metadata": {
        "id": "fRbYvpfhxl9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata_path = \"/content/drive/MyDrive/GestaltMatcher/data/GestaltMatcherDB/v1.1.0/gmdb_metadata/image_metadata_v1.1.0.tsv\"\n",
        "image_metadata = pd.read_csv(metadata_path, sep='\\t')\n",
        "\n",
        "id_to_syndrome = image_metadata.set_index('image_id')['internal_syndrome_name'].to_dict()\n",
        "\n",
        "encodings_df['syndrome_name'] = encodings_df['img_name'].map(id_to_syndrome)\n",
        "\n",
        "print(encodings_df.head())\n",
        "\n",
        "updated_encodings_path = \"/content/drive/MyDrive/GestaltMatcher/data/GestaltMatcherDB/v1.1.0/gmdb_align/all_encodings_with_syndrome_name.csv\"\n",
        "\n",
        "encodings_df.to_csv(updated_encodings_path, index=False)\n",
        "\n",
        "print(f\"Updated encodings file with syndrome_name saved to {updated_encodings_path}\")"
      ],
      "metadata": {
        "id": "2kbIIsK-xoHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#testing for a single image"
      ],
      "metadata": {
        "id": "oFWZoHvixswA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import cv2\n",
        "from albumentations import Compose, Resize, Normalize, HorizontalFlip\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from ast import literal_eval\n",
        "\n",
        "base_transforms = Compose([\n",
        "    Resize(112, 112),\n",
        "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "flip_transforms = Compose([\n",
        "    HorizontalFlip(p=1.0),\n",
        "    Resize(112, 112),\n",
        "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "def preprocess_image(image_path, transforms):\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    augmented = transforms(image=image)\n",
        "    return augmented['image'].unsqueeze(0)\n",
        "\n",
        "def generate_encodings(models, image_tensor):\n",
        "    encodings = []\n",
        "    for model in models:\n",
        "        with torch.no_grad():\n",
        "            encoding = model(image_tensor.to(device)).cpu().numpy()\n",
        "            encodings.append(encoding.squeeze())\n",
        "    return encodings\n",
        "\n",
        "print(encodings_df['representations'].head())\n",
        "\n",
        "def get_top_n_syndromes(image_path, encodings_df, n=10):\n",
        "    img_tensor = preprocess_image(image_path, base_transforms)\n",
        "    original_encodings = generate_encodings([model_r50, model_r100], img_tensor)\n",
        "    flipped_tensor = preprocess_image(image_path, flip_transforms)\n",
        "    flipped_encodings = generate_encodings([model_r50, model_r100], flipped_tensor)\n",
        "    avg_encodings = [\n",
        "        np.mean([orig, flip], axis=0)\n",
        "        for orig, flip in zip(original_encodings, flipped_encodings)\n",
        "    ]\n",
        "    all_gallery_encodings = np.vstack(encodings_df['representations'].values)\n",
        "    similarities = cosine_similarity([avg_encodings[0]], all_gallery_encodings)[0]\n",
        "    syndrome_similarity_pairs = list(zip(encodings_df['syndrome_name'], similarities))\n",
        "    unique_syndromes = {}\n",
        "    for syndrome, similarity in syndrome_similarity_pairs:\n",
        "        if syndrome not in unique_syndromes:\n",
        "            unique_syndromes[syndrome] = similarity\n",
        "        else:\n",
        "            unique_syndromes[syndrome] = max(unique_syndromes[syndrome], similarity)\n",
        "    sorted_syndromes = sorted(unique_syndromes.items(), key=lambda x: x[1], reverse=True)\n",
        "    top_n_syndromes = [syndrome for syndrome, _ in sorted_syndromes[:n]]\n",
        "    top_n_similarities = [similarity for _, similarity in sorted_syndromes[:n]]\n",
        "    return top_n_syndromes, top_n_similarities\n",
        "\n",
        "image_path = \"/content/drive/MyDrive/GestaltMatcher/data/GestaltMatcherDB/v1.1.0/gmdb_align/10000_aligned.jpg\"\n",
        "top_1_syndromes, top_1_similarities = get_top_n_syndromes(image_path, encodings_df, n=1)\n",
        "top_5_syndromes, top_5_similarities = get_top_n_syndromes(image_path, encodings_df, n=5)\n",
        "top_10_syndromes, top_10_similarities = get_top_n_syndromes(image_path, encodings_df, n=10)\n",
        "\n",
        "print(\"Top-1 Syndrome:\", top_1_syndromes)\n",
        "print(\"Top-5 Syndromes:\", top_5_syndromes)\n",
        "print(\"Top-10 Syndromes:\", top_10_syndromes)"
      ],
      "metadata": {
        "id": "F4ut_iDFx0Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Web App Embedded code Starts here"
      ],
      "metadata": {
        "id": "OpXNZ8e-x834"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask\n",
        "!pip install flask-ngrok\n",
        "!pip install pyngrok\n",
        "!pip install flask pyngrok albumentations torch pandas numpy opencv-python-headless\n",
        "checkpoint = torch.load(final_model_path_r50, map_location=device)\n",
        "print(\"Checkpoint keys:\", checkpoint.keys())\n",
        "\n",
        "\n",
        "print(\"Checking 'encodings_df' columns before adding 'syndrome_name':\")\n",
        "print(encodings_df.columns)\n",
        "\n",
        "#Adding syndrome name if missing\n",
        "if \"syndrome_name\" not in encodings_df.columns:\n",
        "    metadata_path = \"/content/drive/MyDrive/GestaltMatcher/data/GestaltMatcherDB/v1.1.0/gmdb_metadata/image_metadata_v1.1.0.tsv\"\n",
        "    image_metadata = pd.read_csv(metadata_path, sep='\\t')\n",
        "    id_to_syndrome = image_metadata.set_index('image_id')['internal_syndrome_name'].to_dict()\n",
        "    encodings_df['syndrome_name'] = encodings_df['img_name'].map(id_to_syndrome)\n",
        "\n",
        "print(\"Checking 'encodings_df' columns after adding 'syndrome_name':\")\n",
        "print(encodings_df.columns)\n",
        "print(encodings_df.head())\n",
        "\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Set your Ngrok authtoken\n",
        "ngrok.set_auth_token(\"put you ngork authtoken here\")"
      ],
      "metadata": {
        "id": "34JQxYMuyGFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#flask app function"
      ],
      "metadata": {
        "id": "W5eVXVYUykWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "without css"
      ],
      "metadata": {
        "id": "7FzDOIxK_ABL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify, render_template_string\n",
        "from pyngrok import ngrok\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from albumentations import Compose, Normalize, Resize\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "import os\n",
        "from onnx2torch import convert\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "ngrok.set_auth_token(\"your ngork token here\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "arcface_r50_path = \"/content/drive/MyDrive/GestaltMatcher/saved_models/glint360k_r50.onnx\"\n",
        "arcface_r100_path = \"/content/drive/MyDrive/GestaltMatcher/saved_models/glint360k_r100.onnx\"\n",
        "final_model_path_r50 = \"/content/drive/MyDrive/GestaltMatcher/saved_models/model_r50_trained_final.pth\"\n",
        "final_model_path_r100 = \"/content/drive/MyDrive/GestaltMatcher/saved_models/model_r100_trained_final.pth\"\n",
        "\n",
        "class ArcFaceClassifier(torch.nn.Module):\n",
        "    def __init__(self, backbone_path, num_classes, device=\"cuda\"):\n",
        "        super(ArcFaceClassifier, self).__init__()\n",
        "        self.backbone = convert(backbone_path).to(device)\n",
        "        self.classifier = torch.nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            features = self.backbone(x)\n",
        "        out = self.classifier(features)\n",
        "        return out\n",
        "\n",
        "def load_model(model, checkpoint_path, num_classes):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    checkpoint_num_classes = checkpoint[\"classifier.weight\"].size(0)\n",
        "    if num_classes != checkpoint_num_classes:\n",
        "        model.classifier = torch.nn.Linear(512, checkpoint_num_classes).to(device)\n",
        "    model.load_state_dict(checkpoint, strict=False)\n",
        "    return model\n",
        "\n",
        "num_classes = 275\n",
        "model_r50 = ArcFaceClassifier(arcface_r50_path, num_classes, device=device).to(device)\n",
        "model_r50 = load_model(model_r50, final_model_path_r50, num_classes)\n",
        "model_r50.eval()\n",
        "\n",
        "model_r100 = ArcFaceClassifier(arcface_r100_path, num_classes, device=device).to(device)\n",
        "model_r100 = load_model(model_r100, final_model_path_r100, num_classes)\n",
        "model_r100.eval()\n",
        "\n",
        "encodings_path = \"/content/drive/MyDrive/GestaltMatcher/data/GestaltMatcherDB/v1.1.0/gmdb_align/all_encodings_with_syndrome_name.csv\"\n",
        "encodings_df = pd.read_csv(encodings_path)\n",
        "encodings_df[\"representations\"] = encodings_df[\"representations\"].apply(eval)\n",
        "\n",
        "if 'syndrome_name' not in encodings_df.columns:\n",
        "    metadata_path = \"/content/drive/MyDrive/GestaltMatcher/data/GestaltMatcherDB/v1.1.0/gmdb_metadata/image_metadata_v1.1.0.tsv\"\n",
        "    image_metadata = pd.read_csv(metadata_path, sep='\\t')\n",
        "    id_to_syndrome = image_metadata.set_index('image_id')['internal_syndrome_name'].to_dict()\n",
        "    encodings_df['syndrome_name'] = encodings_df['img_name'].map(id_to_syndrome)\n",
        "\n",
        "if encodings_df[\"syndrome_name\"].isnull().sum() > 0:\n",
        "    raise ValueError(\"The 'syndrome_name' column contains missing values.\")\n",
        "\n",
        "base_transforms = Compose([\n",
        "    Resize(112, 112),\n",
        "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "def preprocess_image(image_path, transforms):\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    augmented = transforms(image=image)\n",
        "    return augmented['image'].unsqueeze(0)\n",
        "\n",
        "def generate_encodings(models, image_tensor):\n",
        "    encodings = []\n",
        "    for model in models:\n",
        "        with torch.no_grad():\n",
        "            encoding = model(image_tensor.to(device)).cpu().numpy()\n",
        "            encodings.append(encoding.squeeze())\n",
        "    return np.mean(encodings, axis=0)\n",
        "\n",
        "def get_top_n_syndromes(image_path, encodings_df, n=10):\n",
        "    image_tensor = preprocess_image(image_path, base_transforms)\n",
        "    models = [model_r50, model_r100]\n",
        "    query_encoding = generate_encodings(models, image_tensor)\n",
        "\n",
        "    gallery_encodings = np.vstack(encodings_df[\"representations\"].values)\n",
        "    similarities = np.dot(gallery_encodings, query_encoding) / (\n",
        "        np.linalg.norm(gallery_encodings, axis=1) * np.linalg.norm(query_encoding)\n",
        "    )\n",
        "\n",
        "    top_n_indices = np.argsort(similarities)[::-1][:n]\n",
        "    top_n_syndromes = encodings_df.iloc[top_n_indices][\"syndrome_name\"].values\n",
        "    top_n_similarities = similarities[top_n_indices]\n",
        "    return top_n_syndromes, top_n_similarities\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    html = \"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "        <title>Gestalt Matcher</title>\n",
        "        <style>\n",
        "            body { font-family: Arial, sans-serif; text-align: center; margin: 50px; }\n",
        "            input[type=\"file\"] { margin: 20px 0; }\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1>Upload an Image for Syndrome Prediction</h1>\n",
        "        <form action=\"/predict\" method=\"post\" enctype=\"multipart/form-data\">\n",
        "            <input type=\"file\" name=\"file\" />\n",
        "            <button type=\"submit\">Submit</button>\n",
        "        </form>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    return render_template_string(html)\n",
        "\n",
        "@app.route(\"/predict\", methods=[\"POST\"])\n",
        "def predict():\n",
        "    if \"file\" not in request.files:\n",
        "        return render_template_string(\"<h1>Error: No file provided</h1><a href='/'>Go back</a>\"), 400\n",
        "\n",
        "    file = request.files[\"file\"]\n",
        "    if file.filename == \"\":\n",
        "        return render_template_string(\"<h1>Error: No file selected</h1><a href='/'>Go back</a>\"), 400\n",
        "\n",
        "    image_path = \"/content/uploaded_image.jpg\"\n",
        "    file.save(image_path)\n",
        "\n",
        "    try:\n",
        "        top_10_syndromes, top_10_similarities = get_top_n_syndromes(image_path, encodings_df, n=10)\n",
        "        results = [{\"syndrome\": syndrome, \"similarity\": similarity}\n",
        "                   for syndrome, similarity in zip(top_10_syndromes, top_10_similarities)]\n",
        "        html_results = f\"\"\"\n",
        "        <!DOCTYPE html>\n",
        "        <html>\n",
        "        <head>\n",
        "            <title>Results</title>\n",
        "        </head>\n",
        "        <body>\n",
        "            <h1>Top-10 Predicted Syndromes</h1>\n",
        "            <ul>\n",
        "                {\"\".join([f\"<li><strong>{r['syndrome']}</strong>: {r['similarity']:.4f}</li>\" for r in results])}\n",
        "            </ul>\n",
        "            <a href=\"/\">Go back</a>\n",
        "        </body>\n",
        "        </html>\n",
        "        \"\"\"\n",
        "        return render_template_string(html_results)\n",
        "    except Exception as e:\n",
        "        error_message = f\"<h1>Error: {str(e)}</h1><a href='/'>Go back</a>\"\n",
        "        return render_template_string(error_message), 500\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(f\"Public URL: {public_url}\")\n",
        "    app.run(port=5000)"
      ],
      "metadata": {
        "id": "-BPtFpUDykip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Flask-SQLAlchemy"
      ],
      "metadata": {
        "id": "6CQGvo-e0VXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#with css"
      ],
      "metadata": {
        "id": "BpkYrTNm-9lK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify, render_template_string\n",
        "from pyngrok import ngrok\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from albumentations import Compose, Normalize, Resize\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "import os\n",
        "from onnx2torch import convert\n",
        "\n",
        "app = Flask(__name__)\n",
        "ngrok.set_auth_token(\"2poeuBY1WjxBOxfakPm3xBtMCVM_2FiQjjAAEbuTb4JXhuBqR\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "arcface_r50_path = \"/content/drive/MyDrive/GestaltMatcher/saved_models/glint360k_r50.onnx\"\n",
        "arcface_r100_path = \"/content/drive/MyDrive/GestaltMatcher/saved_models/glint360k_r100.onnx\"\n",
        "final_model_path_r50 = \"/content/drive/MyDrive/GestaltMatcher/saved_models/model_r50_trained_final.pth\"\n",
        "final_model_path_r100 = \"/content/drive/MyDrive/GestaltMatcher/saved_models/model_r100_trained_final.pth\"\n",
        "\n",
        "class ArcFaceClassifier(torch.nn.Module):\n",
        "    def __init__(self, backbone_path, num_classes, device=\"cuda\"):\n",
        "        super(ArcFaceClassifier, self).__init__()\n",
        "        self.backbone = convert(backbone_path).to(device)\n",
        "        self.classifier = torch.nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            features = self.backbone(x)\n",
        "        out = self.classifier(features)\n",
        "        return out\n",
        "\n",
        "def load_model(model, checkpoint_path, num_classes):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    checkpoint_num_classes = checkpoint[\"classifier.weight\"].size(0)\n",
        "    if num_classes != checkpoint_num_classes:\n",
        "        model.classifier = torch.nn.Linear(512, checkpoint_num_classes).to(device)\n",
        "    model.load_state_dict(checkpoint, strict=False)\n",
        "    return model\n",
        "\n",
        "num_classes = 275\n",
        "model_r50 = ArcFaceClassifier(arcface_r50_path, num_classes, device=device).to(device)\n",
        "model_r50 = load_model(model_r50, final_model_path_r50, num_classes)\n",
        "model_r50.eval()\n",
        "\n",
        "model_r100 = ArcFaceClassifier(arcface_r100_path, num_classes, device=device).to(device)\n",
        "model_r100 = load_model(model_r100, final_model_path_r100, num_classes)\n",
        "model_r100.eval()\n",
        "\n",
        "encodings_path = \"/content/drive/MyDrive/GestaltMatcher/data/GestaltMatcherDB/v1.1.0/gmdb_align/all_encodings_with_syndrome_name.csv\"\n",
        "encodings_df = pd.read_csv(encodings_path)\n",
        "encodings_df[\"representations\"] = encodings_df[\"representations\"].apply(eval)\n",
        "\n",
        "if 'syndrome_name' not in encodings_df.columns:\n",
        "    metadata_path = \"/content/drive/MyDrive/GestaltMatcher/data/GestaltMatcherDB/v1.1.0/gmdb_metadata/image_metadata_v1.1.0.tsv\"\n",
        "    image_metadata = pd.read_csv(metadata_path, sep='\\t')\n",
        "    id_to_syndrome = image_metadata.set_index('image_id')['internal_syndrome_name'].to_dict()\n",
        "    encodings_df['syndrome_name'] = encodings_df['img_name'].map(id_to_syndrome)\n",
        "\n",
        "if encodings_df[\"syndrome_name\"].isnull().sum() > 0:\n",
        "    raise ValueError(\"The 'syndrome_name' column contains missing values.\")\n",
        "\n",
        "base_transforms = Compose([\n",
        "    Resize(112, 112),\n",
        "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "def preprocess_image(image_path, transforms):\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    augmented = transforms(image=image)\n",
        "    return augmented['image'].unsqueeze(0)\n",
        "\n",
        "def generate_encodings(models, image_tensor):\n",
        "    encodings = []\n",
        "    for model in models:\n",
        "        with torch.no_grad():\n",
        "            encoding = model(image_tensor.to(device)).cpu().numpy()\n",
        "            encodings.append(encoding.squeeze())\n",
        "    return np.mean(encodings, axis=0)\n",
        "\n",
        "def get_top_n_syndromes(image_path, encodings_df, n=10):\n",
        "    image_tensor = preprocess_image(image_path, base_transforms)\n",
        "    models = [model_r50, model_r100]\n",
        "    query_encoding = generate_encodings(models, image_tensor)\n",
        "\n",
        "    gallery_encodings = np.vstack(encodings_df[\"representations\"].values)\n",
        "    similarities = np.dot(gallery_encodings, query_encoding) / (\n",
        "        np.linalg.norm(gallery_encodings, axis=1) * np.linalg.norm(query_encoding)\n",
        "    )\n",
        "\n",
        "    top_n_indices = np.argsort(similarities)[::-1][:n]\n",
        "    top_n_syndromes = encodings_df.iloc[top_n_indices][\"syndrome_name\"].values\n",
        "    top_n_similarities = similarities[top_n_indices]\n",
        "    return top_n_syndromes, top_n_similarities\n",
        "\n",
        "glassmorphism_css = \"\"\"\n",
        "body {\n",
        "    font-family: Arial, sans-serif;\n",
        "    display: flex;\n",
        "    flex-direction: column;\n",
        "    align-items: center;\n",
        "    justify-content: center;\n",
        "    height: 100vh;\n",
        "    margin: 0;\n",
        "    background: linear-gradient(135deg, #74ebd5, #9face6);\n",
        "    overflow: hidden;\n",
        "}\n",
        ".glass-container {\n",
        "    background: rgba(255, 255, 255, 0.15);\n",
        "    border-radius: 16px;\n",
        "    box-shadow: 0 4px 30px rgba(0, 0, 0, 0.1);\n",
        "    backdrop-filter: blur(10px);\n",
        "    -webkit-backdrop-filter: blur(10px);\n",
        "    border: 1px solid rgba(255, 255, 255, 0.3);\n",
        "    padding: 20px;\n",
        "    max-width: 500px;\n",
        "    width: 90%;\n",
        "    text-align: center;\n",
        "}\n",
        "button {\n",
        "    background-color: #4CAF50;\n",
        "    border: none;\n",
        "    color: white;\n",
        "    padding: 10px 20px;\n",
        "    text-align: center;\n",
        "    text-decoration: none;\n",
        "    display: inline-block;\n",
        "    font-size: 16px;\n",
        "    border-radius: 8px;\n",
        "    cursor: pointer;\n",
        "    margin-top: 20px;\n",
        "    transition: background-color 0.3s ease;\n",
        "}\n",
        "button:hover {\n",
        "    background-color: #45a049;\n",
        "}\n",
        "input[type=\"file\"] {\n",
        "    margin: 20px 0;\n",
        "    padding: 10px;\n",
        "    font-size: 16px;\n",
        "}\n",
        "ul {\n",
        "    list-style-type: none;\n",
        "    padding: 0;\n",
        "}\n",
        "li {\n",
        "    margin: 5px 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    html = f\"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "        <title>Gestalt Matcher</title>\n",
        "        <style>{glassmorphism_css}</style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <div class=\"glass-container\">\n",
        "            <h1>Upload an Image for Syndrome Prediction</h1>\n",
        "            <form action=\"/predict\" method=\"post\" enctype=\"multipart/form-data\">\n",
        "                <input type=\"file\" name=\"file\" />\n",
        "                <button type=\"submit\">Submit</button>\n",
        "            </form>\n",
        "        </div>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    return render_template_string(html)\n",
        "\n",
        "@app.route(\"/predict\", methods=[\"POST\"])\n",
        "def predict():\n",
        "    if \"file\" not in request.files:\n",
        "        return render_template_string(\"<h1>Error: No file provided</h1><a href='/'>Go back</a>\"), 400\n",
        "\n",
        "    file = request.files[\"file\"]\n",
        "    if file.filename == \"\":\n",
        "        return render_template_string(\"<h1>Error: No file selected</h1><a href='/'>Go back</a>\"), 400\n",
        "\n",
        "    image_path = \"/content/uploaded_image.jpg\"\n",
        "    file.save(image_path)\n",
        "\n",
        "    try:\n",
        "        top_10_syndromes, top_10_similarities = get_top_n_syndromes(image_path, encodings_df, n=10)\n",
        "        results = [{\"syndrome\": syndrome, \"similarity\": similarity}\n",
        "                   for syndrome, similarity in zip(top_10_syndromes, top_10_similarities)]\n",
        "        html_results = f\"\"\"\n",
        "        <!DOCTYPE html>\n",
        "        <html>\n",
        "        <head>\n",
        "            <title>Results</title>\n",
        "            <style>{glassmorphism_css}</style>\n",
        "        </head>\n",
        "        <body>\n",
        "            <div class=\"glass-container\">\n",
        "                <h1>Top-10 Predicted Syndromes</h1>\n",
        "                <ul>\n",
        "                    {\"\".join([f\"<li><strong>{r['syndrome']}</strong>: {r['similarity']:.4f}</li>\" for r in results])}\n",
        "                </ul>\n",
        "                <a href=\"/\"><button>Go back</button></a>\n",
        "            </div>\n",
        "        </body>\n",
        "        </html>\n",
        "        \"\"\"\n",
        "        return render_template_string(html_results)\n",
        "    except Exception as e:\n",
        "        error_message = f\"<h1>Error: {str(e)}</h1><a href='/'>Go back</a>\"\n",
        "        return render_template_string(error_message), 500\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(f\"Public URL: {public_url}\")\n",
        "    app.run(port=5000)"
      ],
      "metadata": {
        "id": "CK3uwE-K0TGv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}